apiVersion: v1
kind: ConfigMap
metadata:
  name: dual-orchestration-monitoring
  namespace: base-ingestion
  labels:
    app.kubernetes.io/name: monitoring-config
    app.kubernetes.io/component: observability
data:
  prometheus-rules.yaml: |
    groups:
    - name: dual-orchestration.rules
      rules:
      
      # Airflow Business Orchestration Metrics
      - alert: AirflowDAGFailureRate
        expr: rate(airflow_dag_run_failed_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: airflow
          orchestration_layer: business
        annotations:
          summary: "Airflow DAG failure rate is high"
          description: "Business orchestration layer showing {{ $value }} failures per second"
          
      - alert: BusinessSLAViolation
        expr: airflow_dag_run_duration_seconds > (airflow_dag_sla_seconds * 1.2)
        for: 2m
        labels:
          severity: critical
          component: airflow
          orchestration_layer: business
        annotations:
          summary: "Business SLA violation detected"
          description: "DAG {{ $labels.dag_id }} exceeded SLA by {{ $value }} seconds"
          
      # Argo Workflows Technical Execution Metrics
      - alert: ArgoWorkflowFailureSpike
        expr: rate(argo_workflows_count{phase="Failed"}[5m]) > 0.05
        for: 3m
        labels:
          severity: warning
          component: argo-workflows
          orchestration_layer: technical
        annotations:
          summary: "Argo Workflows failure spike detected"
          description: "Technical execution layer showing increased failures"
          
      - alert: WorkflowExecutionStalled
        expr: argo_workflow_status_phase{phase="Running"} and on() time() - argo_workflow_created_time > 7200
        for: 10m
        labels:
          severity: critical
          component: argo-workflows
          orchestration_layer: technical
        annotations:
          summary: "Workflow execution appears stalled"
          description: "Workflow {{ $labels.name }} has been running for over 2 hours"
          
      # Integration Health Metrics
      - alert: AirflowArgoIntegrationFailure
        expr: rate(airflow_kubernetes_pod_operator_failures_total[5m]) > 0.02
        for: 5m
        labels:
          severity: critical
          component: integration
          orchestration_layer: dual
        annotations:
          summary: "Airflow-Argo integration failures detected"
          description: "Dual orchestration integration showing communication failures"
          
      # Sophisticated Agent Health
      - alert: DataIngestionAgentDown
        expr: up{job=~"base-data-.*-service"} == 0
        for: 2m
        labels:
          severity: critical
          component: data-ingestion-agents
          orchestration_layer: technical
        annotations:
          summary: "Data ingestion agent is down"
          description: "Agent {{ $labels.job }} is not responding to health checks"
          
      # ML Model Performance
      - alert: MLModelPredictionAccuracy
        expr: ml_model_accuracy{model_type="source_detection"} < 0.8
        for: 10m
        labels:
          severity: warning
          component: ml-models
          orchestration_layer: technical
        annotations:
          summary: "ML model prediction accuracy degraded"
          description: "Source detection model accuracy dropped to {{ $value }}"
          
      # Business Impact Metrics
      - alert: CriticalDataSourceFailure
        expr: |
          (
            sum by (source_name) (airflow_task_fail_count{task_id=~".*bloomberg.*|.*reuters.*|.*critical.*"}) 
            / 
            sum by (source_name) (airflow_task_total_count{task_id=~".*bloomberg.*|.*reuters.*|.*critical.*"})
          ) > 0.1
        for: 1m
        labels:
          severity: critical
          component: business-impact
          orchestration_layer: business
          escalation: "immediate"
        annotations:
          summary: "Critical financial data source failure"
          description: "{{ $labels.source_name }} showing {{ $value }}% failure rate - immediate business impact"
          
  grafana-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Dual Orchestration: Airflow + Argo Workflows",
        "tags": ["data-ingestion", "dual-orchestration"],
        "timezone": "browser",
        "panels": [
          {
            "title": "Business Orchestration Layer (Airflow)",
            "type": "row",
            "gridPos": {"h": 1, "w": 24, "x": 0, "y": 0}
          },
          {
            "title": "DAG Success Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "rate(airflow_dag_run_success_total[1h]) / rate(airflow_dag_run_total[1h])",
                "legendFormat": "Success Rate"
              }
            ],
            "gridPos": {"h": 8, "w": 6, "x": 0, "y": 1}
          },
          {
            "title": "Business SLA Compliance",
            "type": "gauge",
            "targets": [
              {
                "expr": "avg(airflow_dag_run_duration_seconds < airflow_dag_sla_seconds)",
                "legendFormat": "SLA Compliance %"
              }
            ],
            "gridPos": {"h": 8, "w": 6, "x": 6, "y": 1}
          },
          {
            "title": "Data Source Processing Status",
            "type": "table",
            "targets": [
              {
                "expr": "airflow_task_duration_seconds",
                "format": "table",
                "legendFormat": ""
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 1}
          },
          {
            "title": "Technical Execution Layer (Argo Workflows)",
            "type": "row",
            "gridPos": {"h": 1, "w": 24, "x": 0, "y": 9}
          },
          {
            "title": "Workflow Execution Success Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "rate(argo_workflows_count{phase=\"Succeeded\"}[1h]) / rate(argo_workflows_count[1h])",
                "legendFormat": "Success Rate"
              }
            ],
            "gridPos": {"h": 8, "w": 6, "x": 0, "y": 10}
          },
          {
            "title": "Active Workflows by Phase",
            "type": "piechart",
            "targets": [
              {
                "expr": "argo_workflows_count",
                "legendFormat": "{{phase}}"
              }
            ],
            "gridPos": {"h": 8, "w": 6, "x": 6, "y": 10}
          },
          {
            "title": "Agent Performance Metrics",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total{job=~\"base-data-.*-service\"}[5m])",
                "legendFormat": "{{job}} - Requests/sec"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 10}
          },
          {
            "title": "Integration Health",
            "type": "row",
            "gridPos": {"h": 1, "w": 24, "x": 0, "y": 18}
          },
          {
            "title": "Airflow-Argo Integration Latency",
            "type": "graph",
            "targets": [
              {
                "expr": "airflow_kubernetes_pod_operator_duration_seconds",
                "legendFormat": "Integration Latency"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 19}
          },
          {
            "title": "End-to-End Pipeline Duration",
            "type": "graph",
            "targets": [
              {
                "expr": "argo_workflow_info{name=~\"ingestion-.*\"} * on(name) argo_workflow_status_completion_timestamp - on(name) argo_workflow_status_start_timestamp",
                "legendFormat": "{{name}} - Duration"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 19}
          }
        ],
        "time": {"from": "now-6h", "to": "now"},
        "refresh": "30s"
      }
    }
    
  alert-manager-config.yaml: |
    global:
      smtp_smarthost: 'smtp.company.com:587'
      smtp_from: 'alerts@company.com'
      
    route:
      group_by: ['alertname', 'orchestration_layer']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      
      # Critical business alerts - immediate escalation
      - match:
          severity: critical
          component: business-impact
        receiver: 'business-critical-alerts'
        group_wait: 0s
        repeat_interval: 5m
        
      # Technical execution alerts
      - match:
          orchestration_layer: technical
        receiver: 'technical-team-alerts'
        
      # Business orchestration alerts  
      - match:
          orchestration_layer: business
        receiver: 'business-team-alerts'
        
      # Integration alerts
      - match:
          component: integration
        receiver: 'platform-team-alerts'
        
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://alertmanager-webhook:5000/'
        
    - name: 'business-critical-alerts'
      email_configs:
      - to: 'cto@company.com,risk-management@company.com'
        subject: 'CRITICAL: Data Ingestion Business Impact Alert'
        body: |
          Critical data ingestion failure detected:
          
          Alert: {{ .GroupLabels.alertname }}
          Source: {{ .GroupLabels.source_name }}
          Impact: {{ .CommonAnnotations.description }}
          Time: {{ .CommonAnnotations.startsAt }}
          
          Immediate investigation required.
      slack_configs:
      - api_url: '${SLACK_API_URL}'
        channel: '#critical-alerts'
        title: 'CRITICAL Data Ingestion Alert'
        text: 'Critical business impact detected - immediate attention required'
        
    - name: 'technical-team-alerts'
      email_configs:
      - to: 'data-engineering@company.com'
        subject: 'Technical Execution Alert: {{ .GroupLabels.alertname }}'
      slack_configs:
      - api_url: '${SLACK_API_URL}'
        channel: '#data-engineering'
        
    - name: 'business-team-alerts'
      email_configs:
      - to: 'data-operations@company.com'
        subject: 'Business Orchestration Alert: {{ .GroupLabels.alertname }}'
      slack_configs:
      - api_url: '${SLACK_API_URL}'
        channel: '#data-operations'
        
    - name: 'platform-team-alerts'
      email_configs:
      - to: 'platform-engineering@company.com'
        subject: 'Integration Alert: {{ .GroupLabels.alertname }}'
      slack_configs:
      - api_url: '${SLACK_API_URL}'
        channel: '#platform-engineering'