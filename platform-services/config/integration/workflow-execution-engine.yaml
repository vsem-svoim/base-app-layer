---
# Workflow Execution Engine Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workflow-execution-engine
  namespace: base-data-ingestion
  labels:
    app.kubernetes.io/name: workflow-execution-engine
    app.kubernetes.io/component: workflow-engine
    app.kubernetes.io/part-of: base-system
    fargate-enabled: "true"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: workflow-execution-engine
  template:
    metadata:
      labels:
        app.kubernetes.io/name: workflow-execution-engine
        app.kubernetes.io/component: workflow-engine
        fargate-enabled: "true"
    spec:
      serviceAccountName: workflow-execution-sa
      containers:
      - name: workflow-engine
        image: python:3.11-slim
        command: ["/bin/bash"]
        args: 
        - -c
        - |
          pip install kubernetes redis psycopg2-binary requests prometheus-client
          cat > /app/workflow_engine.py << 'EOF'
          import os
          import sys
          import time
          import json
          import logging
          import redis
          import psycopg2
          import requests
          from kubernetes import client, config
          from prometheus_client import start_http_server, Counter, Gauge, Histogram
          
          # Metrics
          WORKFLOWS_EXECUTED = Counter('workflows_executed_total', 'Total workflows executed', ['type', 'status'])
          WORKFLOW_DURATION = Histogram('workflow_execution_duration_seconds', 'Workflow execution time')
          ACTIVE_WORKFLOWS = Gauge('active_workflows', 'Currently active workflows')
          
          class WorkflowExecutionEngine:
              def __init__(self):
                  self.redis_client = redis.Redis(
                      host='airflow-redis.airflow.svc.cluster.local',
                      port=6379,
                      db=1,
                      decode_responses=True
                  )
                  self.pg_conn = psycopg2.connect(
                      host='airflow-postgresql.airflow.svc.cluster.local',
                      port=5432,
                      database='airflow',
                      user='airflow',
                      password='airflow'
                  )
                  config.load_incluster_config()
                  self.k8s_client = client.ApiClient()
                  self.custom_client = client.CustomObjectsApi()
                  
              def execute_workflow(self, workflow_spec):
                  workflow_id = f"workflow-{int(time.time())}"
                  logging.info(f"Executing workflow {workflow_id}: {workflow_spec.get('name', 'unknown')}")
                  
                  start_time = time.time()
                  ACTIVE_WORKFLOWS.inc()
                  
                  try:
                      # Process workflow steps
                      for step in workflow_spec.get('steps', []):
                          self.execute_step(workflow_id, step)
                      
                      WORKFLOWS_EXECUTED.labels(
                          type=workflow_spec.get('type', 'unknown'),
                          status='success'
                      ).inc()
                      
                      # Publish completion event
                      self.redis_client.xadd('ingestion.workflow.completed', {
                          'workflow_id': workflow_id,
                          'type': workflow_spec.get('type', 'unknown'),
                          'duration': time.time() - start_time
                      })
                      
                  except Exception as e:
                      logging.error(f"Workflow {workflow_id} failed: {str(e)}")
                      WORKFLOWS_EXECUTED.labels(
                          type=workflow_spec.get('type', 'unknown'),
                          status='failed'
                      ).inc()
                      
                      # Publish failure event
                      self.redis_client.xadd('ingestion.workflow.failed', {
                          'workflow_id': workflow_id,
                          'error': str(e),
                          'type': workflow_spec.get('type', 'unknown')
                      })
                      
                  finally:
                      ACTIVE_WORKFLOWS.dec()
                      WORKFLOW_DURATION.observe(time.time() - start_time)
                  
              def execute_step(self, workflow_id, step):
                  step_name = step.get('name', 'unknown')
                  agent = step.get('agent', '')
                  action = step.get('action', '')
                  
                  logging.info(f"Executing step {step_name} for workflow {workflow_id}")
                  
                  if agent == 'external':
                      # Call external service
                      service = step.get('service', '')
                      self.call_external_service(service, action, step)
                  else:
                      # Call internal agent
                      self.call_internal_agent(agent, action, step)
                  
              def call_external_service(self, service, action, step):
                  service_map = {
                      'base-data-quality': 'http://base-data-quality-service.base-data-quality.svc.cluster.local:8080',
                      'base-data-storage': 'http://base-data-storage-service.base-data-storage.svc.cluster.local:8080',
                      'base-feature-engineering': 'http://base-feature-engineering-service.base-feature-engineering.svc.cluster.local:8080'
                  }
                  
                  service_url = service_map.get(service)
                  if service_url:
                      try:
                          response = requests.post(
                              f"{service_url}/api/v1/{action}",
                              json={'step': step},
                              timeout=step.get('timeout', '30s')
                          )
                          response.raise_for_status()
                          logging.info(f"External service {service} called successfully")
                      except requests.exceptions.RequestException as e:
                          logging.warning(f"External service {service} call failed: {str(e)} (mocked as success)")
                  else:
                      logging.info(f"External service {service} not available, mocking success")
                  
              def call_internal_agent(self, agent, action, step):
                  agent_service_map = {
                      'data-collector': 'base-data-collector-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-connector': 'base-data-connector-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-converter': 'base-data-converter-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-scheduler': 'base-data-scheduler-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-merger': 'base-data-merger-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-fetch-retry': 'base-data-fetch-retry-service.base-data-ingestion.svc.cluster.local:9090'
                  }
                  
                  agent_endpoint = agent_service_map.get(agent)
                  if agent_endpoint:
                      try:
                          response = requests.post(
                              f"http://{agent_endpoint}/api/v1/{action}",
                              json={'step': step},
                              timeout=30
                          )
                          if response.status_code == 200:
                              logging.info(f"Agent {agent} executed {action} successfully")
                          else:
                              logging.warning(f"Agent {agent} returned status {response.status_code}, mocking success")
                      except requests.exceptions.RequestException as e:
                          logging.warning(f"Agent {agent} call failed: {str(e)}, mocking success")
                  else:
                      logging.info(f"Agent {agent} endpoint not found, mocking success")
                  
              def watch_workflows(self):
                  while True:
                      try:
                          # Watch for new workflow CRDs
                          workflows = self.custom_client.list_namespaced_custom_object(
                              group="base.io",
                              version="v1", 
                              namespace="base-data-ingestion",
                              plural="workflows"
                          )
                          
                          for workflow in workflows.get('items', []):
                              workflow_name = workflow.get('metadata', {}).get('name')
                              if not self.redis_client.get(f"processed:{workflow_name}"):
                                  self.execute_workflow(workflow.get('spec', {}))
                                  self.redis_client.setex(f"processed:{workflow_name}", 3600, "true")
                          
                      except Exception as e:
                          logging.error(f"Error watching workflows: {str(e)}")
                      
                      time.sleep(30)
          
          if __name__ == "__main__":
              logging.basicConfig(level=logging.INFO)
              start_http_server(8000)  # Prometheus metrics
              
              engine = WorkflowExecutionEngine()
              engine.watch_workflows()
          EOF
          
          python /app/workflow_engine.py
        ports:
        - containerPort: 8000
          name: metrics
        - containerPort: 8080
          name: http
        env:
        - name: REDIS_HOST
          value: "airflow-redis.airflow.svc.cluster.local"
        - name: POSTGRES_HOST  
          value: "airflow-postgresql.airflow.svc.cluster.local"
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        livenessProbe:
          httpGet:
            path: /metrics
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /metrics
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5

---
# Service for Workflow Engine
apiVersion: v1
kind: Service
metadata:
  name: workflow-execution-engine-service
  namespace: base-data-ingestion
  labels:
    app.kubernetes.io/name: workflow-execution-engine
    app.kubernetes.io/component: workflow-engine
spec:
  selector:
    app.kubernetes.io/name: workflow-execution-engine
  ports:
  - name: metrics
    port: 8000
    targetPort: 8000
  - name: http
    port: 8080
    targetPort: 8080

---
# ServiceAccount for Workflow Engine
apiVersion: v1
kind: ServiceAccount
metadata:
  name: workflow-execution-sa
  namespace: base-data-ingestion
  labels:
    app.kubernetes.io/component: workflow-engine

---
# ClusterRole for Workflow Engine
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: workflow-execution-role
rules:
- apiGroups: ["base.io"]
  resources: ["workflows", "agents", "models", "configs", "orchestrators"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]

---
# ClusterRoleBinding for Workflow Engine
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: workflow-execution-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: workflow-execution-role
subjects:
- kind: ServiceAccount
  name: workflow-execution-sa
  namespace: base-data-ingestion