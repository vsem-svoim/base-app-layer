---
# Orchestrator Controller Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: orchestrator-controller
  namespace: base-data-ingestion
  labels:
    app.kubernetes.io/name: orchestrator-controller
    app.kubernetes.io/component: orchestration
    app.kubernetes.io/part-of: base-system
    fargate-enabled: "true"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: orchestrator-controller
  template:
    metadata:
      labels:
        app.kubernetes.io/name: orchestrator-controller
        app.kubernetes.io/component: orchestration
        fargate-enabled: "true"
    spec:
      serviceAccountName: orchestrator-controller-sa
      containers:
      - name: controller
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install kubernetes redis psycopg2-binary requests prometheus-client flask
          cat > /app/orchestrator_controller.py << 'EOF'
          import os
          import sys
          import time
          import json
          import logging
          import threading
          import redis
          import psycopg2
          import requests
          from datetime import datetime, timedelta
          from flask import Flask, request, jsonify
          from kubernetes import client, config
          from prometheus_client import start_http_server, Counter, Gauge, Histogram
          
          # Metrics
          ORCHESTRATOR_REQUESTS = Counter('orchestrator_requests_total', 'Total orchestrator requests', ['type', 'status'])
          AGENT_HEALTH = Gauge('agent_health_status', 'Agent health status', ['agent'])
          WORKFLOW_QUEUE_SIZE = Gauge('workflow_queue_size', 'Number of workflows in queue')
          COORDINATION_LATENCY = Histogram('agent_coordination_latency_seconds', 'Agent coordination latency')
          
          app = Flask(__name__)
          
          class OrchestratorController:
              def __init__(self):
                  self.redis_client = redis.Redis(
                      host='airflow-redis.airflow.svc.cluster.local',
                      port=6379,
                      db=2,  # Use different DB for orchestrator
                      decode_responses=True
                  )
                  
                  self.pg_conn = psycopg2.connect(
                      host='airflow-postgresql.airflow.svc.cluster.local',
                      port=5432,
                      database='airflow',
                      user='airflow',
                      password='airflow'
                  )
                  
                  config.load_incluster_config()
                  self.k8s_client = client.ApiClient()
                  self.custom_client = client.CustomObjectsApi()
                  
                  # Service discovery
                  self.agents = {
                      'data-collector': 'base-data-collector-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-connector': 'base-data-connector-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-converter': 'base-data-converter-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-scheduler': 'base-data-scheduler-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-merger': 'base-data-merger-service.base-data-ingestion.svc.cluster.local:9090',
                      'data-fetch-retry': 'base-data-fetch-retry-service.base-data-ingestion.svc.cluster.local:9090'
                  }
                  
                  self.external_services = {
                      'base-data-quality': 'http://base-data-quality-service.base-data-quality.svc.cluster.local:8080',
                      'base-data-storage': 'http://base-data-storage-service.base-data-storage.svc.cluster.local:8080',
                      'base-feature-engineering': 'http://base-feature-engineering-service.base-feature-engineering.svc.cluster.local:8080',
                      'base-data-security': 'http://base-data-security-service.base-data-security.svc.cluster.local:8080'
                  }
                  
                  # Initialize orchestrator state table
                  self.init_database()
                  
                  # Start health monitoring
                  self.start_health_monitoring()
                  
              def init_database(self):
                  """Initialize orchestrator state database"""
                  cursor = self.pg_conn.cursor()
                  try:
                      cursor.execute("""
                          CREATE TABLE IF NOT EXISTS orchestrator_state (
                              id SERIAL PRIMARY KEY,
                              orchestrator_name VARCHAR(255),
                              workflow_id VARCHAR(255),
                              status VARCHAR(50),
                              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                              metadata JSONB
                          )
                      """)
                      
                      cursor.execute("""
                          CREATE TABLE IF NOT EXISTS agent_health (
                              agent_name VARCHAR(255) PRIMARY KEY,
                              status VARCHAR(50),
                              last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                              metadata JSONB
                          )
                      """)
                      
                      self.pg_conn.commit()
                      logging.info("Database initialized successfully")
                  except Exception as e:
                      logging.error(f"Database initialization failed: {str(e)}")
                      self.pg_conn.rollback()
                  finally:
                      cursor.close()
                      
              def start_health_monitoring(self):
                  """Start background health monitoring thread"""
                  def monitor_health():
                      while True:
                          try:
                              self.check_agent_health()
                              self.update_workflow_queue_metrics()
                              time.sleep(30)
                          except Exception as e:
                              logging.error(f"Health monitoring error: {str(e)}")
                              time.sleep(60)
                  
                  health_thread = threading.Thread(target=monitor_health, daemon=True)
                  health_thread.start()
                  
              def check_agent_health(self):
                  """Check health of all agents"""
                  for agent_name, agent_endpoint in self.agents.items():
                      try:
                          start_time = time.time()
                          response = requests.get(
                              f"http://{agent_endpoint}/health",
                              timeout=5
                          )
                          latency = time.time() - start_time
                          COORDINATION_LATENCY.observe(latency)
                          
                          if response.status_code == 200:
                              AGENT_HEALTH.labels(agent=agent_name).set(1)
                              self.update_agent_health(agent_name, 'healthy', response.json())
                          else:
                              AGENT_HEALTH.labels(agent=agent_name).set(0)
                              self.update_agent_health(agent_name, 'unhealthy', {'status_code': response.status_code})
                              
                      except Exception as e:
                          AGENT_HEALTH.labels(agent=agent_name).set(0)
                          self.update_agent_health(agent_name, 'error', {'error': str(e)})
                          
              def update_agent_health(self, agent_name, status, metadata):
                  """Update agent health in database"""
                  cursor = self.pg_conn.cursor()
                  try:
                      cursor.execute("""
                          INSERT INTO agent_health (agent_name, status, metadata)
                          VALUES (%s, %s, %s)
                          ON CONFLICT (agent_name) 
                          DO UPDATE SET 
                              status = EXCLUDED.status,
                              last_seen = CURRENT_TIMESTAMP,
                              metadata = EXCLUDED.metadata
                      """, (agent_name, status, json.dumps(metadata)))
                      self.pg_conn.commit()
                  except Exception as e:
                      logging.error(f"Failed to update agent health: {str(e)}")
                      self.pg_conn.rollback()
                  finally:
                      cursor.close()
                      
              def update_workflow_queue_metrics(self):
                  """Update workflow queue metrics"""
                  try:
                      queue_size = len(self.redis_client.keys("workflow:*"))
                      WORKFLOW_QUEUE_SIZE.set(queue_size)
                  except Exception as e:
                      logging.error(f"Failed to update queue metrics: {str(e)}")
                      
              def coordinate_workflow(self, workflow_spec):
                  """Coordinate workflow execution"""
                  workflow_id = f"workflow-{int(time.time())}"
                  
                  try:
                      # Store workflow in queue
                      self.redis_client.setex(
                          f"workflow:{workflow_id}",
                          3600,  # 1 hour TTL
                          json.dumps(workflow_spec)
                      )
                      
                      # Update state in database
                      self.update_orchestrator_state(workflow_id, 'queued', workflow_spec)
                      
                      # Publish workflow started event
                      self.redis_client.xadd('ingestion.workflow.started', {
                          'workflow_id': workflow_id,
                          'type': workflow_spec.get('type', 'unknown'),
                          'timestamp': datetime.now().isoformat()
                      })
                      
                      ORCHESTRATOR_REQUESTS.labels(type='workflow', status='accepted').inc()
                      return {'workflow_id': workflow_id, 'status': 'queued'}
                      
                  except Exception as e:
                      logging.error(f"Workflow coordination failed: {str(e)}")
                      ORCHESTRATOR_REQUESTS.labels(type='workflow', status='failed').inc()
                      raise
                      
              def update_orchestrator_state(self, workflow_id, status, metadata):
                  """Update orchestrator state in database"""
                  cursor = self.pg_conn.cursor()
                  try:
                      cursor.execute("""
                          INSERT INTO orchestrator_state 
                          (orchestrator_name, workflow_id, status, metadata)
                          VALUES (%s, %s, %s, %s)
                      """, ('ingestion-manager', workflow_id, status, json.dumps(metadata)))
                      self.pg_conn.commit()
                  except Exception as e:
                      logging.error(f"Failed to update orchestrator state: {str(e)}")
                      self.pg_conn.rollback()
                  finally:
                      cursor.close()
          
          # Global orchestrator instance
          orchestrator = OrchestratorController()
          
          @app.route('/health', methods=['GET'])
          def health_check():
              """Health check endpoint"""
              try:
                  # Check Redis connection
                  orchestrator.redis_client.ping()
                  
                  # Check PostgreSQL connection
                  cursor = orchestrator.pg_conn.cursor()
                  cursor.execute("SELECT 1")
                  cursor.close()
                  
                  return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()})
              except Exception as e:
                  return jsonify({'status': 'unhealthy', 'error': str(e)}), 500
                  
          @app.route('/coordinate', methods=['POST'])
          def coordinate_workflow():
              """Coordinate workflow execution"""
              try:
                  workflow_spec = request.json
                  result = orchestrator.coordinate_workflow(workflow_spec)
                  return jsonify(result)
              except Exception as e:
                  return jsonify({'error': str(e)}), 500
                  
          @app.route('/agents/health', methods=['GET'])
          def get_agents_health():
              """Get health status of all agents"""
              cursor = orchestrator.pg_conn.cursor()
              try:
                  cursor.execute("SELECT agent_name, status, last_seen, metadata FROM agent_health")
                  results = cursor.fetchall()
                  
                  agents_health = []
                  for row in results:
                      agents_health.append({
                          'agent': row[0],
                          'status': row[1], 
                          'last_seen': row[2].isoformat(),
                          'metadata': row[3]
                      })
                  
                  return jsonify({'agents': agents_health})
              except Exception as e:
                  return jsonify({'error': str(e)}), 500
              finally:
                  cursor.close()
                  
          @app.route('/workflows/status', methods=['GET'])
          def get_workflows_status():
              """Get status of all workflows"""
              cursor = orchestrator.pg_conn.cursor()
              try:
                  cursor.execute("""
                      SELECT workflow_id, status, created_at, updated_at, metadata 
                      FROM orchestrator_state 
                      ORDER BY created_at DESC 
                      LIMIT 100
                  """)
                  results = cursor.fetchall()
                  
                  workflows = []
                  for row in results:
                      workflows.append({
                          'workflow_id': row[0],
                          'status': row[1],
                          'created_at': row[2].isoformat(),
                          'updated_at': row[3].isoformat(),
                          'metadata': row[4]
                      })
                  
                  return jsonify({'workflows': workflows})
              except Exception as e:
                  return jsonify({'error': str(e)}), 500
              finally:
                  cursor.close()
          
          if __name__ == "__main__":
              logging.basicConfig(level=logging.INFO)
              
              # Start Prometheus metrics server
              start_http_server(9090)
              
              # Start Flask app
              app.run(host='0.0.0.0', port=8080, debug=False)
          EOF
          
          python /app/orchestrator_controller.py
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: integration-config
          mountPath: /etc/integration
          readOnly: true
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: integration-config
        configMap:
          name: orchestrator-integration-config

---
# Service for Orchestrator Controller
apiVersion: v1
kind: Service
metadata:
  name: orchestrator-controller-service
  namespace: base-data-ingestion
  labels:
    app.kubernetes.io/name: orchestrator-controller
    app.kubernetes.io/component: orchestration
spec:
  selector:
    app.kubernetes.io/name: orchestrator-controller
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090

---
# ServiceAccount for Orchestrator Controller
apiVersion: v1
kind: ServiceAccount
metadata:
  name: orchestrator-controller-sa
  namespace: base-data-ingestion
  labels:
    app.kubernetes.io/component: orchestration

---
# ClusterRole for Orchestrator Controller
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: orchestrator-controller-role
rules:
- apiGroups: ["base.io"]
  resources: ["orchestrators", "workflows", "agents", "models", "configs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["services", "endpoints", "pods", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["monitoring.coreos.com"]
  resources: ["servicemonitors"]
  verbs: ["get", "list", "watch", "create", "update"]

---
# ClusterRoleBinding for Orchestrator Controller
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: orchestrator-controller-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: orchestrator-controller-role
subjects:
- kind: ServiceAccount
  name: orchestrator-controller-sa
  namespace: base-data-ingestion